# ğŸ‘‹ Â¡Hola! Soy Gabriel

Un estudiante avanzado de IngenierÃ­a ElectrÃ³nica. A lo largo de mi formaciÃ³n acadÃ©mica he desarrollado una sÃ³lida base en electrÃ³nica, sistemas embebidos y control, complementada con experiencia en:

- ğŸ§ **Sistemas operativos Linux**
- ğŸŒ **Redes de datos y telecomunicaciones**

He tenido la oportunidad de trabajar profesionalmente en sectores clave como:

- âš¡ **EnergÃ­a**
- ğŸ›¢ï¸ **PetrÃ³leo y Gas**
- ğŸ“¡ **Telecomunicaciones**

Grandes empresas han confiado en mis aptitudes tÃ©cnicas y en mi capacidad de aprendizaje, permitiÃ©ndome adquirir experiencia real en entornos exigentes y de alta responsabilidad.

Puedes ver mi CV actualizado [aquÃ­](https://drive.google.com/file/d/1A4DfP_m6mVv7Wqqt68NNqIYuX1zJpULd/view?usp=sharing).

## ğŸš€ TransiciÃ³n hacia la IngenierÃ­a de Datos

Actualmente me encuentro en una etapa de transiciÃ³n profesional hacia el mundo de la **IngenierÃ­a de Datos**, donde he enfocado mi aprendizaje autodidacta en el uso de herramientas modernas y tecnologÃ­as clave del ecosistema de datos.

Stack:

- **Lenguajes de programaciÃ³n**: `Python, SQL`  
- **Procesamiento de datos**: `Pandas, Spark`  
- **ETL & flujos de trabajo**: `Airflow, NiFi`  
- **Nube**: `Cloudera, Azure, Docker`  
- **Bases de datos**: `PostgreSQL`  
- **Otros**: `Git, Linux, Bash`  

## ğŸ“ FormaciÃ³n complementaria

He adquirido habilidades prÃ¡cticas y teÃ³ricas mediante plataformas como:

- ğŸ“˜ **Udemy** y **DataCamp**: Fundamentos de Python, SQL, modelado de datos, pipelines y ETL.
- â˜ï¸ **Microsoft Azure Learning**: Conceptos de cloud computing aplicados a la ingenierÃ­a de datos.
- ğŸ§  **Cloudera**: Conocimientos bÃ¡sicos sobre la plataforma en general.
- âš™ï¸ **AVEVA OSIsoft PI System**: Consumo de datos a travÃ©s de la API para la integraciÃ³n de estos en sistemas SCADA.

## ğŸ“« Contacto

PodÃ©s contactarme a travÃ©s de [LinkedIn](https://linkedin.com/in/gabriel-ulloa-saavedra/) o revisar mis proyectos acÃ¡ en GitHub.

Â¡Gracias por visitar mi portafolio!

# Proyecto nÂ°1

Proceso ETL con Python y SQLite. [Click aquÃ­](https://github.com/ulloa-g/etl_csv_to_sql)

DesarrollÃ© un pipeline ETL sencillo utilizando **Python** para consumir datos desde un archivo **.csv**, realizar tareas de **limpieza y transformaciÃ³n**, y exportarlos a una base de datos **SQLite**. Este proyecto me permitiÃ³ afianzar conceptos clave de manipulaciÃ³n de datos con **pandas**.

**DesafÃ­os abordados:** 
- ValidaciÃ³n de datos inconsistentes y reemplazo de valores no estandarizados
- ConversiÃ³n de tipos de datos para carga en base relacional

# Proyecto nÂ°2

Proceso ETL desde API pÃºblica a PostgreSQL. [Click aquÃ­](https://github.com/ulloa-g/etl_api_to_sql)

ImplementÃ© un pipeline ETL mÃ¡s complejo, donde consumo datos dinÃ¡micos desde una **API pÃºblica**, realizo **limpieza, transformaciÃ³n** y manejo de **valores nulos**, para luego cargar los datos procesados en una base de datos **PostgreSQL**.

Este proyecto me permitiÃ³ trabajar con estructuras de datos anidadas, asegurar integridad en la carga y aplicar **buenas prÃ¡cticas de seguridad**, como la gestiÃ³n de credenciales sensibles mediante archivos de configuraciÃ³n excluidos del repositorio (uso de `.gitignore`).

**DesafÃ­os abordados:**
- Manejo de respuestas JSON complejas y paginadas
- NormalizaciÃ³n y limpieza de estructuras de datos irregulares
- PrevenciÃ³n de exposiciÃ³n de datos sensibles en repositorios pÃºblicos
