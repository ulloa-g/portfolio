# üëã ¬°Hola! Soy Gabriel

Un estudiante avanzado de Ingenier√≠a Electr√≥nica. A lo largo de mi formaci√≥n acad√©mica he desarrollado una s√≥lida base en electr√≥nica, sistemas embebidos y control, complementada con experiencia en:

- üêß **Sistemas operativos Linux**
- üåê **Redes de datos y telecomunicaciones**

He tenido la oportunidad de trabajar profesionalmente en sectores clave como:

- ‚ö° **Energ√≠a**
- üõ¢Ô∏è **Petr√≥leo y Gas**
- üì° **Telecomunicaciones**

Grandes empresas han confiado en mis aptitudes t√©cnicas y en mi capacidad de aprendizaje, permiti√©ndome adquirir experiencia real en entornos exigentes y de alta responsabilidad.

Puedes ver mi CV actualizado [aqu√≠](https://drive.google.com/file/d/1A4DfP_m6mVv7Wqqt68NNqIYuX1zJpULd/view?usp=sharing).

## üöÄ Transici√≥n hacia la Ingenier√≠a de Datos

Actualmente me encuentro en una etapa de transici√≥n profesional hacia el mundo de la **Ingenier√≠a de Datos**, donde he enfocado mi aprendizaje autodidacta en el uso de herramientas modernas y tecnolog√≠as clave del ecosistema de datos.

Stack:

- **Lenguajes de programaci√≥n**: `Python, SQL`  
- **Procesamiento de datos**: `Pandas`  
- **ETL & flujos de trabajo**: `Airflow`  
- **Nube**: `Azure`  
- **Bases de datos**: `PostgreSQL`  
- **Otros**: `Git, Linux, Bash`  

## üéì Formaci√≥n complementaria

He adquirido habilidades pr√°cticas y te√≥ricas mediante plataformas como:

- üìò **Udemy** y **DataCamp**: Fundamentos de Python, SQL, modelado de datos, pipelines y ETL.
- ‚òÅÔ∏è **Microsoft Azure Learning**: Conceptos de cloud computing aplicados a la ingenier√≠a de datos.
- üß† **Cloudera**: Conocimientos b√°sicos sobre la plataforma en general.
- ‚öôÔ∏è **AVEVA OSIsoft PI System**: Consumo de datos a trav√©s de la API para la integraci√≥n de estos en sistemas SCADA.

## üì´ Contacto

Pod√©s contactarme a trav√©s de [LinkedIn](https://linkedin.com/in/gabriel-ulloa-saavedra/) o revisar mis proyectos ac√° en GitHub.

¬°Gracias por visitar mi portafolio!

# Proyecto n¬∞1

Proceso ETL con Python y SQLite. [Click aqu√≠](https://github.com/ulloa-g/etl_csv_to_sql)

Desarroll√© un pipeline ETL sencillo utilizando **Python** para consumir datos desde un archivo **.csv**, realizar tareas de **limpieza y transformaci√≥n**, y exportarlos a una base de datos **SQLite**. Este proyecto me permiti√≥ afianzar conceptos clave de manipulaci√≥n de datos con **pandas**.

**Desaf√≠os abordados:** 
- Validaci√≥n de datos inconsistentes y reemplazo de valores no estandarizados
- Conversi√≥n de tipos de datos para carga en base relacional

# Proyecto n¬∞2

Proceso ETL desde API p√∫blica a PostgreSQL. [Click aqu√≠](https://github.com/ulloa-g/etl_api_to_sql)

Implement√© un pipeline ETL m√°s complejo, donde consumo datos din√°micos desde una **API p√∫blica**, realizo **limpieza, transformaci√≥n** y manejo de **valores nulos**, para luego cargar los datos procesados en una base de datos **PostgreSQL**.

Este proyecto me permiti√≥ trabajar con estructuras de datos anidadas, asegurar integridad en la carga y aplicar **buenas pr√°cticas de seguridad**, como la gesti√≥n de credenciales sensibles mediante archivos de configuraci√≥n excluidos del repositorio (uso de `.gitignore`).

**Desaf√≠os abordados:**
- Manejo de respuestas JSON complejas y paginadas
- Normalizaci√≥n y limpieza de estructuras de datos irregulares
- Prevenci√≥n de exposici√≥n de datos sensibles en repositorios p√∫blicos

# Proyecto n¬∞3

An√°lisis Exploratorio y Modelo de Regresi√≥n Lineal. [Click aqu√≠](https://github.com/ulloa-g/exploratory_data_analysis)

Realic√© un **an√°lisis exploratorio de datos (EDA)** utilizando **Jupyter Notebook**, empleando librer√≠as como **NumPy**, **pandas**, **requests** y **SciPy**. El objetivo fue identificar patrones, inconsistencias y preparar los datos.

Durante el proceso se abordaron tareas como:
- Identificaci√≥n y correcci√≥n de **inconsistencias de datos**
- **Conversi√≥n de unidades de ingenier√≠a** y tipos de datos
- Transformaci√≥n de variables categ√≥ricas en **datos num√©ricos**

Finalizado el an√°lisis, se construy√≥ un **modelo de regresi√≥n lineal** como aproximaci√≥n inicial al aprendizaje autom√°tico, permitiendo establecer relaciones entre variables y evaluar su comportamiento.

# Proyecto n¬∞4

Pipeline en la nube con Azure Data Factory. [Click aqu√≠](https://github.com/ulloa-g/azure_datafactory)

Desarroll√© un pipeline en la nube utilizando **Azure Data Factory**, desde la creaci√≥n de la instancia hasta la configuraci√≥n de los recursos necesarios para la ingesti√≥n y almacenamiento de datos estructurados.

El flujo consisti√≥ en la extracci√≥n de datos desde un archivo **.CSV alojado en Azure Blob Storage** y carga en una base de datos **Azure SQL Database**. Se utiliz√≥ el editor de consultas de Azure para crear la tabla destino y definir su estructura.


**Desaf√≠os abordados:**
- Integraci√≥n entre servicios de Azure
- Configuraci√≥n de conexiones y control de acceso

# Proyecto n¬∞5

Pipeline con Airflow y carga a SQLite. [Click aqu√≠](https://github.com/ulloa-g/etl_airflow)

Implement√© un pipeline de datos utilizando un dataset descargado desde **Kaggle**, donde se realizaron **transformaciones b√°sicas** y limpieza inicial para preparar los datos. Posteriormente fueron cargados en una base de datos **SQLite**.

La novedad principal del proyecto fue la **orquestaci√≥n completa del proceso mediante Apache Airflow**, definiendo DAGs y tareas, dependencias y control de ejecuci√≥n.

**Desaf√≠os abordados:**
- Automatizaci√≥n
- Separaci√≥n de l√≥gica de transformaci√≥n y l√≥gica de orquestaci√≥n

# Proyecto n¬∞6

ETL con Apache Nifi. [Click aqu√≠](https://github.com/ulloa-g/etl_apache_nifi)

El objetivo principal es demostrar el conocimiento pr√°ctico de los componentes y funcionalidades clave de Apache NiFi en el contexto de un pipeline de Extracci√≥n, Transformaci√≥n y Carga (ETL). Se busca evidenciar la capacidad de dise√±ar, construir y operar flujos de datos utilizando esta herramienta, creando un flujo de datos que lee archivos de un sistema de archivos local, los transforma a√±adiendo un timestamp y reemplazando texto, y luego guarda el archivo transformado en formato CSV

**Desafios abordados:**
- Dise√±ar, construir y operar flujos de datos de manera modular y organizada utilizando un "Process Group"
- Gestionar el acceso y la seguridad b√°sica de la interfaz de usuario de NiFi, incluyendo la generaci√≥n de credenciales de usuario √∫nicas.

## üß≠ Estado actual

Actualmente me encuentro profundizando mis conocimientos en herramientas y stack utilizado en mi portfolio y aprendiendo sobre **AWS**.


Mi objetivo es seguir construyendo una base s√≥lida en herramientas modernas de ingenier√≠a de datos y aplicarlas en contextos reales.
